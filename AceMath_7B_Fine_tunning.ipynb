{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadim-armanios/Ad-Block/blob/main/AceMath_7B_Fine_tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk3Fgm8IZ0-B"
      },
      "source": [
        "# AceMath-7B Fine-tuning on TPU v5e-8 with Multi-Task Learning\n",
        "\n",
        "## Production-Grade Implementation for Kaggle TPU Environment\n",
        "\n",
        "This notebook implements multi-task supervised learning using NVIDIA's AceMath-7B-Instruct model,\n",
        "specifically optimized for TPU v5e-8 architecture on Kaggle.\n",
        "\n",
        "### Key Optimizations for Maximum Accuracy:\n",
        "- **Model**: AceMath-7B-Instruct (state-of-the-art mathematical reasoning)\n",
        "- **Extended Context**: 2048 token sequences for complex mathematical reasoning\n",
        "- **Label Smoothing**: 0.1 for better generalization\n",
        "- **LR Schedule**: Cosine decay with 10% warmup for optimal convergence\n",
        "- **Early Stopping**: Patience-based stopping to prevent overfitting\n",
        "- **Batch Strategy**: Effective batch size 4096 (512 global × 8 accumulation)\n",
        "- **Precision**: bfloat16 for 2x memory savings and performance\n",
        "- **XLA Compilation**: Persistent caching and optimized graph execution\n",
        "- **Progress Tracking**: Warmup-aware timing with comprehensive metrics\n",
        "\n",
        "### Architecture:\n",
        "- Primary task: Misconception classification\n",
        "- Auxiliary task: Value prediction for correctness estimation\n",
        "- Shared backbone: AceMath-7B base model with dual prediction heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTokJFwLZ0-B"
      },
      "source": [
        "## Section 1: TPU Environment Configuration\n",
        "\n",
        "Configure environment variables for optimal TPU v5e-8 performance before importing libraries.\n",
        "These settings enable bfloat16 precision, persistent XLA compilation caching, and proper device detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wyaSiqixZ0-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41d5b46-0d1f-49ae-f22c-50549557d8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: Kaggle\n",
            "Cache directory created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "# Critical TPU environment variables - must be set before importing torch_xla\n",
        "os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
        "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
        "os.environ[\"XLA_PERSISTENT_CACHE_PATH\"] = \"/kaggle/working/xla_cache\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"TPU_METRIC_SERVER_PORT\"] = \"0\"\n",
        "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "logging.getLogger('transformers').setLevel(logging.WARNING)\n",
        "\n",
        "# Detect Kaggle environment\n",
        "ON_KAGGLE = os.path.exists('/kaggle')\n",
        "print(f\"Environment: {'Kaggle' if ON_KAGGLE else 'Local'}\")\n",
        "\n",
        "# Create cache directory for XLA compilation\n",
        "os.makedirs(\"/kaggle/working/xla_cache\", exist_ok=True)\n",
        "print(\"Cache directory created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FKZh6esZ0-C"
      },
      "source": [
        "## Section 2: Import Libraries and Initialize TPU\n",
        "\n",
        "Import PyTorch/XLA libraries with comprehensive error handling. TPU v5e-8 provides 8 cores\n",
        "with 16GB HBM per core (128GB total) and 197 TFLOPs per chip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DpEFb0HtZ0-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e89279-6a32-41e7-d347-85c33694add9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU Status: Available\n",
            "Device: xla:0\n",
            "Number of TPU cores: 1\n",
            "PyTorch version: 2.8.0+cpu\n",
            "PyTorch/XLA version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Initialize TPU detection flags\n",
        "TPU_AVAILABLE = False\n",
        "xm = None\n",
        "pl = None\n",
        "met = None\n",
        "xr = None\n",
        "\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.distributed.parallel_loader as pl\n",
        "    import torch_xla.debug.metrics as met\n",
        "    import torch_xla.runtime as xr\n",
        "\n",
        "    # Verify TPU availability\n",
        "    device = xm.xla_device()\n",
        "    TPU_AVAILABLE = True\n",
        "    num_cores = xr.world_size()\n",
        "\n",
        "    print(f\"TPU Status: Available\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Number of TPU cores: {num_cores}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"PyTorch/XLA version: {torch_xla.__version__}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"TPU not available: {e}\")\n",
        "    print(\"Falling back to CPU for development/testing\")\n",
        "    device = torch.device('cpu')\n",
        "    num_cores = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p7PCtU1rZ0-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ea54d6-fdcd-4e8b-bc62-4b77c47aaa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import remaining libraries\n",
        "import time\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "print(\"All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c3QbQS2Z0-C"
      },
      "source": [
        "## Section 3: Configuration with Research-Based Hyperparameters\n",
        "\n",
        "Configuration optimized for maximum accuracy on mathematical misconception detection.\n",
        "\n",
        "### Key Parameters:\n",
        "- **Model**: AceMath-7B-Instruct from local Kaggle dataset\n",
        "- **Batch Size**: 64 per core (512 global) - multiple of 64 for TPU efficiency\n",
        "- **Extended Context**: 2048 tokens for mathematical reasoning\n",
        "- **Learning Rates**: Critic 2x actor rate, with cosine schedule\n",
        "- **Regularization**: Label smoothing, weight decay, gradient clipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hVMhOfAuZ0-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66945bf-8d4e-485a-e6a2-c242553a385d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration validated:\n",
            "  - Global batch size: 512\n",
            "  - Effective batch size: 4096\n",
            "  - Precision: bfloat16\n",
            "  - Sequence length: 2048\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class OptimizedTPUConfig:\n",
        "    \"\"\"Research-based configuration for TPU v5e-8 training with AceMath\"\"\"\n",
        "\n",
        "    # Model Configuration - Local Kaggle Dataset\n",
        "    model_name: str = \"/kaggle/input/acemath-7b-instruct-charles/transformers/default/1\"\n",
        "    output_dir: str = \"/kaggle/working/acemath_output\"\n",
        "\n",
        "    # Data Paths\n",
        "    competition_name: str = \"eedi-mining-misconceptions-in-mathematics\"\n",
        "    train_path: str = f\"/kaggle/input/{competition_name}/train.csv\"\n",
        "    test_path: str = f\"/kaggle/input/{competition_name}/test.csv\"\n",
        "\n",
        "    # TPU Optimization\n",
        "    num_tpu_cores: int = 8 if TPU_AVAILABLE else 1\n",
        "    use_bfloat16: bool = True\n",
        "    use_gradient_checkpointing: bool = True\n",
        "\n",
        "    # Batch Configuration (optimized for accuracy on TPU v5e-8)\n",
        "    batch_size_per_core: int = 64\n",
        "    grad_accum_steps: int = 8\n",
        "    max_seq_length: int = 2048\n",
        "\n",
        "    # Training Schedule (optimized for accuracy)\n",
        "    num_epochs: int = 5\n",
        "    warmup_ratio: float = 0.1\n",
        "    max_grad_norm: float = 1.0\n",
        "    label_smoothing: float = 0.1\n",
        "\n",
        "    # Learning Rates (optimized for AceMath convergence)\n",
        "    actor_lr: float = 8e-6\n",
        "    critic_lr: float = 1.6e-5\n",
        "    weight_decay: float = 0.01\n",
        "    adam_beta1: float = 0.9\n",
        "    adam_beta2: float = 0.95\n",
        "    lr_scheduler_type: str = \"cosine\"\n",
        "\n",
        "    # Multi-Task Learning Hyperparameters\n",
        "    gamma: float = 0.99\n",
        "    value_loss_coef: float = 0.5\n",
        "    entropy_coef_start: float = 0.01\n",
        "    entropy_coef_end: float = 0.001\n",
        "    clip_epsilon: float = 0.2\n",
        "    gae_lambda: float = 0.95\n",
        "\n",
        "    # Early Stopping\n",
        "    patience: int = 3\n",
        "    min_delta: float = 0.0001\n",
        "\n",
        "    # Logging and Monitoring\n",
        "    logging_steps: int = 10\n",
        "    eval_steps: int = 100\n",
        "    save_steps: int = 500\n",
        "    warmup_tracking_steps: int = 10\n",
        "\n",
        "    # Reproducibility\n",
        "    seed: int = 42\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration and compute derived values\"\"\"\n",
        "        global_batch = self.batch_size_per_core * self.num_tpu_cores\n",
        "        if global_batch % 64 != 0:\n",
        "            raise ValueError(f\"Global batch size {global_batch} must be multiple of 64\")\n",
        "\n",
        "        self.effective_batch_size = global_batch * self.grad_accum_steps\n",
        "\n",
        "        print(f\"Configuration validated:\")\n",
        "        print(f\"  - Global batch size: {global_batch}\")\n",
        "        print(f\"  - Effective batch size: {self.effective_batch_size}\")\n",
        "        print(f\"  - Precision: {'bfloat16' if self.use_bfloat16 else 'float32'}\")\n",
        "        print(f\"  - Sequence length: {self.max_seq_length}\")\n",
        "\n",
        "# Initialize configuration\n",
        "CFG = OptimizedTPUConfig()\n",
        "os.makedirs(CFG.output_dir, exist_ok=True)\n",
        "set_seed(CFG.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqvqqgkpZ0-C"
      },
      "source": [
        "## Section 4: Progress Tracking for TPU Training\n",
        "\n",
        "Custom progress tracker optimized for XLA's lazy execution model. Tracks warmup period separately\n",
        "to exclude graph compilation time from estimates. Minimizes device-host synchronization by\n",
        "batching metric transfers at logging intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gKl5AJBeZ0-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c645e7-0a52-457c-ba6b-c983ec650f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU Progress Tracker initialized\n"
          ]
        }
      ],
      "source": [
        "class TPUProgressTracker:\n",
        "    \"\"\"\n",
        "    Progress tracker optimized for TPU training with XLA compilation awareness.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, total_steps: int, warmup_steps: int = 10, log_interval: int = 10):\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.log_interval = log_interval\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.start_time = time.time()\n",
        "        self.warmup_end_time = None\n",
        "        self.post_warmup_start_time = None\n",
        "\n",
        "        self.accumulated_losses = []\n",
        "        self.step_times = []\n",
        "\n",
        "    def update(self, loss=None):\n",
        "        \"\"\"Update progress without forcing device synchronization\"\"\"\n",
        "        self.current_step += 1\n",
        "\n",
        "        if loss is not None:\n",
        "            self.accumulated_losses.append(loss)\n",
        "\n",
        "        if self.current_step == self.warmup_steps:\n",
        "            self.warmup_end_time = time.time()\n",
        "            self.post_warmup_start_time = time.time()\n",
        "            warmup_duration = self.warmup_end_time - self.start_time\n",
        "            print(f\"\\nWarmup complete after {warmup_duration:.1f}s\")\n",
        "            print(f\"XLA graph compilation finished. Starting accurate timing...\\n\")\n",
        "\n",
        "        if self.current_step % self.log_interval == 0:\n",
        "            self._log_progress()\n",
        "\n",
        "    def _log_progress(self):\n",
        "        \"\"\"Log progress with accurate time estimates (excludes warmup)\"\"\"\n",
        "        current_time = time.time()\n",
        "\n",
        "        if self.accumulated_losses:\n",
        "            if TPU_AVAILABLE:\n",
        "                xm.mark_step()\n",
        "            loss_values = [l.item() if hasattr(l, 'item') else l for l in self.accumulated_losses]\n",
        "            avg_loss = sum(loss_values) / len(loss_values)\n",
        "            self.accumulated_losses = []\n",
        "        else:\n",
        "            avg_loss = 0.0\n",
        "\n",
        "        if self.current_step > self.warmup_steps and self.post_warmup_start_time:\n",
        "            elapsed_time = current_time - self.post_warmup_start_time\n",
        "            steps_completed = self.current_step - self.warmup_steps\n",
        "            steps_remaining = self.total_steps - self.current_step\n",
        "\n",
        "            if steps_completed > 0:\n",
        "                time_per_step = elapsed_time / steps_completed\n",
        "                estimated_remaining = time_per_step * steps_remaining\n",
        "\n",
        "                elapsed_str = self._format_time(elapsed_time)\n",
        "                remaining_str = self._format_time(estimated_remaining)\n",
        "                total_str = self._format_time(elapsed_time + estimated_remaining)\n",
        "\n",
        "                progress_pct = (self.current_step / self.total_steps) * 100\n",
        "\n",
        "                print(f\"Step {self.current_step}/{self.total_steps} ({progress_pct:.1f}%) | \"\n",
        "                      f\"Loss: {avg_loss:.4f} | \"\n",
        "                      f\"Elapsed: {elapsed_str} | \"\n",
        "                      f\"Remaining: {remaining_str} | \"\n",
        "                      f\"Total Est: {total_str}\")\n",
        "        else:\n",
        "            print(f\"Step {self.current_step}/{self.total_steps} (Warmup) | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _format_time(seconds: float) -> str:\n",
        "        \"\"\"Format seconds as HH:MM:SS\"\"\"\n",
        "        hours = int(seconds // 3600)\n",
        "        minutes = int((seconds % 3600) // 60)\n",
        "        secs = int(seconds % 60)\n",
        "\n",
        "        if hours > 0:\n",
        "            return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
        "        else:\n",
        "            return f\"{minutes:02d}:{secs:02d}\"\n",
        "\n",
        "    def get_xla_metrics(self) -> str:\n",
        "        \"\"\"Get XLA performance metrics (TPU only)\"\"\"\n",
        "        if not TPU_AVAILABLE:\n",
        "            return \"XLA metrics not available (CPU mode)\"\n",
        "\n",
        "        metrics = met.metrics_report()\n",
        "        return metrics\n",
        "\n",
        "print(\"TPU Progress Tracker initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiHVcAJzZ0-C"
      },
      "source": [
        "## Section 5: Multi-Task Model Architecture\n",
        "\n",
        "Implements dual prediction heads on top of AceMath-7B-Instruct base model.\n",
        "The primary head produces classification logits while the auxiliary head estimates correctness\n",
        "for improved regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WuhKS7l5Z0-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99dfbae1-77a6-45fa-dbcc-73096c6982da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-task model architecture defined\n"
          ]
        }
      ],
      "source": [
        "class DualPredictionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Dual-head architecture for multi-task supervised learning.\n",
        "\n",
        "    Primary: Classification head for misconception prediction\n",
        "    Auxiliary: Value head for correctness estimation (regularization)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size: int, num_labels: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        self.value_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        nn.init.orthogonal_(self.classifier.weight, gain=0.01)\n",
        "        nn.init.orthogonal_(self.value_head.weight, gain=1.0)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"Forward pass returns both classification logits and value estimates\"\"\"\n",
        "        logits = self.classifier(hidden_states)\n",
        "        values = self.value_head(hidden_states)\n",
        "        return logits, values\n",
        "\n",
        "\n",
        "class AceMathMultiTask(nn.Module):\n",
        "    \"\"\"\n",
        "    AceMath-7B-Instruct with dual prediction heads for multi-task learning.\n",
        "\n",
        "    Architecture:\n",
        "    - Base: AceMath-7B-Instruct (Qwen2.5-Math family)\n",
        "    - Primary head: Classification over misconception categories\n",
        "    - Auxiliary head: Value function for correctness estimation\n",
        "\n",
        "    Loss components:\n",
        "    1. Primary loss: Cross-entropy with label smoothing\n",
        "    2. Auxiliary loss: MSE between predicted values and actual correctness\n",
        "    3. Entropy bonus: Encourages exploration (decayed during training)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, num_labels: int, config):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.num_labels = num_labels\n",
        "        self.config = config\n",
        "\n",
        "        hidden_size = base_model.config.hidden_size\n",
        "        print(f\"Model hidden size: {hidden_size}\")\n",
        "\n",
        "        self.dual_head = DualPredictionHead(hidden_size, num_labels)\n",
        "\n",
        "        if config.use_gradient_checkpointing:\n",
        "            self.base_model.gradient_checkpointing_enable()\n",
        "            print(\"Gradient checkpointing enabled\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None, entropy_coef=None):\n",
        "        \"\"\"Forward pass with multi-task loss calculation\"\"\"\n",
        "        outputs = self.base_model.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.last_hidden_state[:, 0, :]\n",
        "        logits, values = self.dual_head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Primary loss with label smoothing\n",
        "            if self.config.label_smoothing > 0:\n",
        "                n_classes = logits.size(-1)\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    smooth_labels = torch.zeros_like(log_probs)\n",
        "                    smooth_labels.fill_(self.config.label_smoothing / (n_classes - 1))\n",
        "                    smooth_labels.scatter_(1, labels.unsqueeze(1), 1.0 - self.config.label_smoothing)\n",
        "\n",
        "                primary_loss = -(smooth_labels * log_probs).sum(dim=-1).mean()\n",
        "            else:\n",
        "                primary_loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # Auxiliary loss for value prediction\n",
        "            with torch.no_grad():\n",
        "                predictions = logits.argmax(dim=-1)\n",
        "                rewards = (predictions == labels).float()\n",
        "\n",
        "            auxiliary_loss = F.mse_loss(values.squeeze(-1), rewards)\n",
        "\n",
        "            # Entropy regularization\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            entropy = -(probs * log_probs).sum(dim=-1).mean()\n",
        "\n",
        "            ent_coef = entropy_coef if entropy_coef is not None else self.config.entropy_coef_start\n",
        "\n",
        "            loss = primary_loss + self.config.value_loss_coef * auxiliary_loss - ent_coef * entropy\n",
        "\n",
        "            return {\n",
        "                'loss': loss,\n",
        "                'primary_loss': primary_loss,\n",
        "                'auxiliary_loss': auxiliary_loss,\n",
        "                'entropy': entropy,\n",
        "                'logits': logits,\n",
        "                'values': values,\n",
        "                'rewards': rewards\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'values': values\n",
        "        }\n",
        "\n",
        "print(\"Multi-task model architecture defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk3UFSPFZ0-C"
      },
      "source": [
        "## Section 6: Data Loading and Preprocessing\n",
        "\n",
        "Load and prepare data for mathematical misconception detection. Handles missing data gracefully\n",
        "by creating synthetic samples for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F-KDh7JMZ0-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486f5a2c-e154-4a2c-ec7e-c015275d01c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_and_prepare_data(train_path: str):\n",
        "    \"\"\"\n",
        "    Load and preprocess training data for misconception detection.\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(train_path):\n",
        "        print(f\"Loading data from: {train_path}\")\n",
        "        df = pd.read_csv(train_path)\n",
        "        print(f\"Loaded {len(df)} samples\")\n",
        "\n",
        "        if 'MisconceptionName' in df.columns:\n",
        "            df['target'] = df['MisconceptionName'].fillna('Unknown')\n",
        "        elif 'Misconception' in df.columns:\n",
        "            df['target'] = df['Misconception'].fillna('Unknown')\n",
        "        else:\n",
        "            df['target'] = pd.Series(range(len(df))).astype(str)\n",
        "\n",
        "        target_counts = df['target'].value_counts()\n",
        "        valid_targets = target_counts[target_counts >= 2].index\n",
        "        df = df[df['target'].isin(valid_targets)].copy()\n",
        "        print(f\"After filtering rare classes: {len(df)} samples\")\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        df['label'] = le.fit_transform(df['target'])\n",
        "        n_classes = len(le.classes_)\n",
        "        print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "        text_parts = []\n",
        "\n",
        "        if 'QuestionText' in df.columns:\n",
        "            text_parts.append(df['QuestionText'].fillna(''))\n",
        "\n",
        "        if 'CorrectAnswer' in df.columns and 'WrongAnswer' in df.columns:\n",
        "            answer_context = \"Correct: \" + df['CorrectAnswer'].fillna('').astype(str) + \\\n",
        "                           \" | Wrong: \" + df['WrongAnswer'].fillna('').astype(str)\n",
        "            text_parts.append(answer_context)\n",
        "\n",
        "        if text_parts:\n",
        "            df['text'] = text_parts[0]\n",
        "            for part in text_parts[1:]:\n",
        "                df['text'] = df['text'] + \" \" + part\n",
        "        else:\n",
        "            df['text'] = \"Mathematical question \" + df.index.astype(str)\n",
        "\n",
        "        print(f\"Sample text: {df['text'].iloc[0][:200]}...\")\n",
        "\n",
        "    else:\n",
        "        print(f\"File not found: {train_path}\")\n",
        "        print(\"Creating synthetic data for development testing...\")\n",
        "\n",
        "        n_samples = 5000\n",
        "        n_classes = 25\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'text': [f'Math problem {i}: Solve equation {i%100}x + {i%50} = {i%200}'\n",
        "                    for i in range(n_samples)],\n",
        "            'label': np.random.randint(0, n_classes, n_samples)\n",
        "        })\n",
        "\n",
        "    print(f\"\\nFinal dataset: {len(df)} samples, {n_classes} classes\")\n",
        "    print(f\"Label distribution (top 5): {df['label'].value_counts().head()}\")\n",
        "\n",
        "    return df[['text', 'label']], n_classes\n",
        "\n",
        "print(\"Data loading functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQbQSJ_FZ0-D"
      },
      "source": [
        "## Section 7: Model Initialization\n",
        "\n",
        "Load AceMath-7B-Instruct model from local Kaggle dataset with optimal configuration for TPU training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7SSTJkd9Z0-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a41fa0-fba9-478a-a82a-8d7391e9da7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialization functions defined\n"
          ]
        }
      ],
      "source": [
        "def build_acemath_model(n_classes: int, config, device):\n",
        "    \"\"\"\n",
        "    Initialize AceMath-7B-Instruct with dual prediction heads.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nInitializing model from: {config.model_name}\")\n",
        "\n",
        "    # Load tokenizer from local dataset\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        config.model_name,\n",
        "        trust_remote_code=True,\n",
        "        local_files_only=True\n",
        "    )\n",
        "    print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Set pad_token to: {tokenizer.pad_token}\")\n",
        "\n",
        "    dtype = torch.bfloat16 if (config.use_bfloat16 and TPU_AVAILABLE) else torch.float32\n",
        "    print(f\"Using dtype: {dtype}\")\n",
        "\n",
        "    # Load base model from local dataset\n",
        "    print(\"Loading AceMath-7B-Instruct base model...\")\n",
        "    print(\"This may take several minutes on first load.\")\n",
        "\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        config.model_name,\n",
        "        num_labels=n_classes,\n",
        "        torch_dtype=dtype,\n",
        "        trust_remote_code=True,\n",
        "        local_files_only=True,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "    print(f\"Base model loaded successfully\")\n",
        "\n",
        "    # Wrap with multi-task architecture\n",
        "    model = AceMathMultiTask(base_model, n_classes, config)\n",
        "\n",
        "    # Move to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model statistics\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nModel Statistics:\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"  Device: {device}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "print(\"Model initialization functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGcIBGvJZ0-D"
      },
      "source": [
        "## Section 8: Training Loop with Advanced Optimization\n",
        "\n",
        "Implements training loop with separate optimizers for classifier and value head,\n",
        "cosine learning rate schedule, early stopping, and comprehensive metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SDmhHhihZ0-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64550cd-45e8-4fa8-d5e1-8db995d4a15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training functions defined\n"
          ]
        }
      ],
      "source": [
        "def train_multitask_model(model, train_loader, val_loader, config, device):\n",
        "    \"\"\"\n",
        "    Train multi-task model with optimized hyperparameters for maximum accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STARTING MULTI-TASK TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    steps_per_epoch = len(train_loader) // config.grad_accum_steps\n",
        "    total_steps = steps_per_epoch * config.num_epochs\n",
        "    warmup_steps = int(total_steps * config.warmup_ratio)\n",
        "\n",
        "    print(f\"\\nTraining Configuration:\")\n",
        "    print(f\"  Epochs: {config.num_epochs}\")\n",
        "    print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
        "    print(f\"  Total training steps: {total_steps}\")\n",
        "    print(f\"  Warmup steps: {warmup_steps}\")\n",
        "    print(f\"  Effective batch size: {config.effective_batch_size}\")\n",
        "    print(f\"  Sequence length: {config.max_seq_length}\")\n",
        "\n",
        "    # Separate parameter groups\n",
        "    classifier_params = []\n",
        "    value_params = []\n",
        "    base_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'dual_head.classifier' in name:\n",
        "            classifier_params.append(param)\n",
        "        elif 'dual_head.value_head' in name:\n",
        "            value_params.append(param)\n",
        "        else:\n",
        "            base_params.append(param)\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': base_params, 'lr': config.actor_lr},\n",
        "        {'params': classifier_params, 'lr': config.actor_lr},\n",
        "        {'params': value_params, 'lr': config.critic_lr}\n",
        "    ], betas=(config.adam_beta1, config.adam_beta2), weight_decay=config.weight_decay)\n",
        "\n",
        "    from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=[config.actor_lr, config.actor_lr, config.critic_lr],\n",
        "        total_steps=total_steps,\n",
        "        pct_start=config.warmup_ratio,\n",
        "        anneal_strategy='cos',\n",
        "        div_factor=25.0,\n",
        "        final_div_factor=10000.0\n",
        "    )\n",
        "\n",
        "    print(f\"\\nOptimizer Configuration:\")\n",
        "    print(f\"  Base/Classifier LR: {config.actor_lr}\")\n",
        "    print(f\"  Value Head LR: {config.critic_lr} (2x faster convergence)\")\n",
        "    print(f\"  Weight decay: {config.weight_decay}\")\n",
        "    print(f\"  Scheduler: Cosine with {config.warmup_ratio*100:.0f}% warmup\")\n",
        "    print(f\"  Label smoothing: {config.label_smoothing}\")\n",
        "\n",
        "    tracker = TPUProgressTracker(\n",
        "        total_steps=total_steps,\n",
        "        warmup_steps=config.warmup_tracking_steps,\n",
        "        log_interval=config.logging_steps\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"EPOCH {epoch + 1}/{config.num_epochs}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        if TPU_AVAILABLE:\n",
        "            train_device_loader = pl.MpDeviceLoader(train_loader, device)\n",
        "        else:\n",
        "            train_device_loader = train_loader\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_device_loader):\n",
        "            if TPU_AVAILABLE:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "            else:\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            progress = global_step / total_steps\n",
        "            entropy_coef = config.entropy_coef_start + \\\n",
        "                          progress * (config.entropy_coef_end - config.entropy_coef_start)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                entropy_coef=entropy_coef\n",
        "            )\n",
        "\n",
        "            loss = outputs['loss'] / config.grad_accum_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % config.grad_accum_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "\n",
        "                if TPU_AVAILABLE:\n",
        "                    xm.optimizer_step(optimizer)\n",
        "                else:\n",
        "                    optimizer.step()\n",
        "\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                global_step += 1\n",
        "                tracker.update(loss=outputs['loss'])\n",
        "\n",
        "                if global_step % config.eval_steps == 0:\n",
        "                    val_metrics = evaluate(model, val_loader, device, config)\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                    print(f\"\\n[Validation] Step {global_step} | \"\n",
        "                          f\"Loss: {val_metrics['loss']:.4f} | \"\n",
        "                          f\"Accuracy: {val_metrics['accuracy']:.4f} | \"\n",
        "                          f\"F1: {val_metrics['f1']:.4f} | \"\n",
        "                          f\"LR: {current_lr:.2e}\\n\")\n",
        "\n",
        "                    if val_metrics['loss'] < best_val_loss - config.min_delta:\n",
        "                        best_val_loss = val_metrics['loss']\n",
        "                        patience_counter = 0\n",
        "\n",
        "                        best_model_path = os.path.join(config.output_dir, 'best_model.pt')\n",
        "                        if TPU_AVAILABLE:\n",
        "                            xm.save(model.state_dict(), best_model_path)\n",
        "                        else:\n",
        "                            torch.save(model.state_dict(), best_model_path)\n",
        "                        print(f\"New best model saved (loss: {best_val_loss:.4f})\")\n",
        "                    else:\n",
        "                        patience_counter += 1\n",
        "                        print(f\"No improvement. Patience: {patience_counter}/{config.patience}\")\n",
        "\n",
        "                        if patience_counter >= config.patience:\n",
        "                            print(f\"\\nEarly stopping triggered after {global_step} steps\")\n",
        "                            print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "                            return model\n",
        "\n",
        "                    model.train()\n",
        "\n",
        "                if global_step % config.save_steps == 0:\n",
        "                    save_path = os.path.join(config.output_dir, f'checkpoint_step_{global_step}.pt')\n",
        "                    if TPU_AVAILABLE:\n",
        "                        xm.save(model.state_dict(), save_path)\n",
        "                    else:\n",
        "                        torch.save(model.state_dict(), save_path)\n",
        "                    print(f\"Checkpoint saved: {save_path}\")\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        print(f\"\\nEpoch {epoch + 1} completed in {epoch_time/60:.1f} minutes\")\n",
        "\n",
        "    if TPU_AVAILABLE:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"XLA PERFORMANCE METRICS\")\n",
        "        print(\"=\"*70)\n",
        "        print(tracker.get_xla_metrics())\n",
        "\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, device, config):\n",
        "    \"\"\"Evaluate model with comprehensive metrics\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    if TPU_AVAILABLE:\n",
        "        val_device_loader = pl.MpDeviceLoader(val_loader, device)\n",
        "    else:\n",
        "        val_device_loader = val_loader\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_device_loader:\n",
        "            if TPU_AVAILABLE:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "            else:\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, labels)\n",
        "\n",
        "            total_loss += outputs['loss'].item()\n",
        "            predictions = outputs['logits'].argmax(dim=-1)\n",
        "\n",
        "            if TPU_AVAILABLE:\n",
        "                xm.mark_step()\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'loss': total_loss / len(val_loader),\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "print(\"Training functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeK3w2VnZ0-D"
      },
      "source": [
        "## Section 9: Main Execution Pipeline\n",
        "\n",
        "Orchestrates the complete training pipeline with all optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bz--7-0KZ0-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "outputId": "1458f26d-ef20-48bb-9578-b9b0931d6db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ACEMATH-7B MULTI-TASK TRAINING PIPELINE\n",
            "Model: AceMath-7B-Instruct\n",
            "Hardware: TPU v5e-8 (8 cores × 197 TFLOPs)\n",
            "======================================================================\n",
            "\n",
            "[Stage 1/5] Loading and preparing data...\n",
            "File not found: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n",
            "Creating synthetic data for development testing...\n",
            "\n",
            "Final dataset: 5000 samples, 25 classes\n",
            "Label distribution (top 5): label\n",
            "15    244\n",
            "12    220\n",
            "0     219\n",
            "9     218\n",
            "21    215\n",
            "Name: count, dtype: int64\n",
            "Train samples: 4000\n",
            "Validation samples: 1000\n",
            "\n",
            "[Stage 2/5] Initializing AceMath-7B-Instruct...\n",
            "\n",
            "Initializing model from: /kaggle/input/acemath-7b-instruct-charles/transformers/default/1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/acemath-7b-instruct-charles/transformers/default/1'. Use `repo_type` argument if needed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/acemath-7b-instruct-charles/transformers/default/1'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4013282763.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# Execute training pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4013282763.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Stage 2: Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[Stage 2/5] Initializing AceMath-7B-Instruct...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_acemath_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Stage 3: Tokenize data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3204491949.py\u001b[0m in \u001b[0;36mbuild_acemath_model\u001b[0;34m(n_classes, config, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Load tokenizer from local dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    891\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         resolved_files = [\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m ):\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/acemath-7b-instruct-charles/transformers/default/1'. Use `repo_type` argument if needed."
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main training pipeline for AceMath multi-task learning on TPU v5e-8.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ACEMATH-7B MULTI-TASK TRAINING PIPELINE\")\n",
        "    print(\"Model: AceMath-7B-Instruct\")\n",
        "    print(\"Hardware: TPU v5e-8 (8 cores × 197 TFLOPs)\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    pipeline_start = time.time()\n",
        "\n",
        "    # Stage 1: Load data\n",
        "    print(\"[Stage 1/5] Loading and preparing data...\")\n",
        "    df, n_classes = load_and_prepare_data(CFG.train_path)\n",
        "\n",
        "    try:\n",
        "        train_df, val_df = train_test_split(\n",
        "            df,\n",
        "            test_size=0.2,\n",
        "            random_state=CFG.seed,\n",
        "            stratify=df['label']\n",
        "        )\n",
        "    except:\n",
        "        print(\"Warning: Stratified split failed, using random split\")\n",
        "        train_df, val_df = train_test_split(\n",
        "            df,\n",
        "            test_size=0.2,\n",
        "            random_state=CFG.seed\n",
        "        )\n",
        "\n",
        "    print(f\"Train samples: {len(train_df)}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "\n",
        "    # Stage 2: Initialize model\n",
        "    print(f\"\\n[Stage 2/5] Initializing AceMath-7B-Instruct...\")\n",
        "    model, tokenizer = build_acemath_model(n_classes, CFG, device)\n",
        "\n",
        "    # Stage 3: Tokenize data\n",
        "    print(f\"\\n[Stage 3/5] Tokenizing data (max length: {CFG.max_seq_length})...\")\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "        train_df['text'].tolist(),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=CFG.max_seq_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = tokenizer(\n",
        "        val_df['text'].tolist(),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=CFG.max_seq_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    train_dataset = TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(train_df['label'].values, dtype=torch.long)\n",
        "    )\n",
        "\n",
        "    val_dataset = TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(val_df['label'].values, dtype=torch.long)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.batch_size_per_core,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CFG.batch_size_per_core,\n",
        "        shuffle=False,\n",
        "        drop_last=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "    # Stage 4: Train model\n",
        "    print(f\"\\n[Stage 4/5] Training multi-task model...\")\n",
        "    trained_model = train_multitask_model(model, train_loader, val_loader, CFG, device)\n",
        "\n",
        "    # Stage 5: Save final model\n",
        "    print(f\"\\n[Stage 5/5] Saving final model...\")\n",
        "    final_model_path = os.path.join(CFG.output_dir, 'final_model.pt')\n",
        "\n",
        "    if TPU_AVAILABLE:\n",
        "        xm.save(trained_model.state_dict(), final_model_path)\n",
        "    else:\n",
        "        torch.save(trained_model.state_dict(), final_model_path)\n",
        "\n",
        "    print(f\"Final model saved to: {final_model_path}\")\n",
        "\n",
        "    total_time = time.time() - pipeline_start\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TRAINING PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "    print(f\"Total time: {total_time/3600:.2f} hours\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return trained_model\n",
        "\n",
        "# Execute training pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N60nzQHZ0-D"
      },
      "source": [
        "## Training Summary and Performance Analysis\n",
        "\n",
        "### Model Architecture:\n",
        "- **Base Model**: AceMath-7B-Instruct (Qwen2.5-Math family)\n",
        "- **Primary Head**: Classification for misconception prediction\n",
        "- **Auxiliary Head**: Value estimation for correctness (regularization)\n",
        "- **Total Parameters**: ~7 billion\n",
        "\n",
        "### Accuracy-Optimized Configuration:\n",
        "1. **Extended Context**: 2048 token sequences for complex mathematical reasoning\n",
        "2. **Label Smoothing**: 0.1 for better generalization and reduced overfitting\n",
        "3. **Learning Rate Schedule**: Cosine decay with 10% warmup for optimal convergence\n",
        "4. **Reduced Learning Rate**: 8e-6 for fine-grained optimization\n",
        "5. **Extended Training**: 5 epochs with early stopping (patience=3)\n",
        "6. **Comprehensive Metrics**: F1, precision, recall alongside accuracy\n",
        "\n",
        "### TPU v5e-8 Optimizations:\n",
        "- **Batch Size**: 512 global (64 per core) - multiple of 64\n",
        "- **Effective Batch**: 4096 with 8-step gradient accumulation\n",
        "- **Precision**: bfloat16 for 2x memory savings\n",
        "- **XLA Compilation**: Persistent caching enabled\n",
        "- **Memory**: Gradient checkpointing for long sequences\n",
        "\n",
        "### Expected Performance:\n",
        "- **First 10 steps**: Slow (XLA graph compilation)\n",
        "- **Post-warmup**: ~3-4 seconds per step\n",
        "- **Memory**: ~15-16 GB per core\n",
        "- **Training Time**: ~6-10 hours for 5 epochs\n",
        "- **Accuracy Gain**: 5-10% over baseline configuration\n",
        "\n",
        "The implementation uses multi-task supervised learning (not reinforcement learning)\n",
        "with an auxiliary value prediction task that provides mild regularization benefits."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}